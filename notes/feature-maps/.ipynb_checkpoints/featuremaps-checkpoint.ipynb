{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/anaconda3/lib/python3.8/site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21 in /usr/local/anaconda3/lib/python3.8/site-packages (1.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    " # check for convolutional layer\n",
    "     if 'conv' in layer.name:\n",
    " # get filter weights\n",
    "         filters, biases = layer.get_weights()\n",
    "         print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature map of first conv layer for given image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_model = Model(inputs=model.inputs, outputs=model.layers[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fmap_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "User 3x3 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3x3 filter, 3 channels, 64 filters\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8601116 , 0.81635594, 0.47662497],\n",
       "       [0.7392438 , 0.55478704, 0.23765364],\n",
       "       [0.47966576, 0.31966972, 0.25103468]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    " # get the filter\n",
    " f = filters[:, :, :, i]\n",
    " # plot each channel separately\n",
    " for j in range(3):\n",
    " # specify subplot and turn of axis\n",
    "     ax = plt.subplot(n_filters, 3, ix)\n",
    "     ax.set_xticks([])\n",
    "     ax.set_yticks([])\n",
    " # plot filter channel in grayscale\n",
    "     plt.imshow(f[:, :, j], cmap='gray')\n",
    "     ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "The filter across the rows can sometimes differ.\n",
    "Dark squares indicate small weights.\n",
    "Light squares indicate large weights.\n",
    "\n",
    "### Example\n",
    "The first row of filters will detect a gradient from light in top left to dark in the bottom right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd convolutional layer\n",
    "Looking at filters in the second convolutional layer, we can see that again we have 64 filters, but each has 64 channels to match the input feature maps. To see all 64 channels in a row for all 64 filters would require (64×64) 4,096 subplots in which it may be challenging to see any detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation maps **called feature maps** show the result of applying the filters to inputs.\n",
    "The input could be an input image or another feature map.\n",
    "\n",
    "We can visualise what features are detected by a feature map. Certain feature maps will detect small details in the earlier layers and they'll capture more larger patterns closer to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('feature-maps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img', '.ipynb_checkpoints', 'featuremaps.ipynb']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/img/' \n",
    "filename = 'bird.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 block1_conv1 (None, 224, 224, 64)\n",
      "2 block1_conv2 (None, 224, 224, 64)\n",
      "4 block2_conv1 (None, 112, 112, 128)\n",
      "5 block2_conv2 (None, 112, 112, 128)\n",
      "7 block3_conv1 (None, 56, 56, 256)\n",
      "8 block3_conv2 (None, 56, 56, 256)\n",
      "9 block3_conv3 (None, 56, 56, 256)\n",
      "11 block4_conv1 (None, 28, 28, 512)\n",
      "12 block4_conv2 (None, 28, 28, 512)\n",
      "13 block4_conv3 (None, 28, 28, 512)\n",
      "15 block5_conv1 (None, 14, 14, 512)\n",
      "16 block5_conv2 (None, 14, 14, 512)\n",
      "17 block5_conv3 (None, 14, 14, 512)\n"
     ]
    }
   ],
   "source": [
    "# summarize feature map size for each conv layer\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from matplotlib import pyplot\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# summarize feature map shapes\n",
    "for i in range(len(model.layers)):\n",
    "     layer = model.layers[i]\n",
    " # check for convolutional layer\n",
    "     if 'conv' not in layer.name:\n",
    "         continue\n",
    " # summarize output shape\n",
    "     else:\n",
    "         print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmp_model = Model(inputs=model.inputs, outputs=model.layers[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the img\n",
    "IMG_SIZE= 224\n",
    "img = load_img(filepath + filename, target_size = (IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
